{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import pysam\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "# import pyranges as pr\n",
    "import pickle\n",
    "from scipy import interpolate\n",
    "from Bio import SeqIO\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import concurrent.futures\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"calculate the probability of a read arising from each of its references.\")\n",
    "# genome or txome. Currently need txome to find the polyA/T sites\n",
    "# parser.add_argument(\"spliced_reads_genome_bam\", type=str, help=\"genome-based BAM file containing the classified spliced reads\")\n",
    "# parser.add_argument(\"unspliced_reads_genome_bam\", type=str, help=\"genome-based BAM file containing the classified spliced reads\")\n",
    "parser.add_argument(\"double_e_spliced_reads_txome_bam\", type=str, help=\"transcriptome-based BAM filecontaining the classified spliced reads\")\n",
    "parser.add_argument(\"ee_junctional_spliced_reads_txome_bam\", type=str, help=\"transcriptome-based BAM filecontaining the classified spliced reads\")\n",
    "parser.add_argument(\"unspliced_reads_txome_bam\", type=str, help=\"transcriptome-based BAM filecontaining the classified unspliced reads\")\n",
    "parser.add_argument(\"read2origin_tsv\", type=str, help=\"two column tsv file containing the read name and its origin\")\n",
    "parser.add_argument(\"spline_model_file\", type=str,\n",
    "                    help=\"fragment length spline model parameters in a pickle file\")\n",
    "parser.add_argument(\"mlp_model_file\", type=str,\n",
    "                    help=\"trained MLP model for predicting the priming site in a pickle file\")\n",
    "parser.add_argument(\"spliceu_fasta_file\", type=str,\n",
    "                    help=\"spliceu reference sequence fasta file\")\n",
    "parser.add_argument(\"t2g_file\", type=str,\n",
    "                    help=\"transcript to gene mapping tsv file\")\n",
    "parser.add_argument(\"max_frag_len\", type=int, default=500, help=\"polyA minimum length\")\n",
    "parser.add_argument(\"num_threads\", type=int, default=2, help=\"number of threads\")\n",
    "parser.add_argument(\"outdir\", type=str, help=\"output directory\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "gse = \"GSE125188\" \n",
    "srr = \"SRR8448139\"\n",
    "# srr_dir = \"/fs/nexus-projects/sc_frag_len/nextflow/simulation/not_fund_ambiguous_reads/simu_STAR_out\"\n",
    "# srr_dir = \"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simu_classify_out\"\n",
    "srr_dir = f\"/fs/nexus-projects/sc_frag_len/nextflow/workflow_output/process_data/definitive_reads_by_r1/{gse}/{srr}\"\n",
    "args = parser.parse_args([\n",
    "    f\"{srr_dir}/r2_exonic_r1_another_exon_spliced_txome.bam\",\n",
    "    f\"{srr_dir}/r2_exonic_r1_ee_junction_spliced_txome.bam\",\n",
    "    f\"{srr_dir}/r2_exonic_r1_unspliced_txome.bam\",\n",
    "    f\"{srr_dir}/unigene_reads_to_gene.tsv\",\n",
    "    \"/fs/nexus-projects/sc_frag_len/nextflow/fit_model/Jan8_newout/spline_model.pkl\",\n",
    "    \"/fs/nexus-projects/sc_frag_len/nextflow/test/mlp/mlp.pkl\",\n",
    "    \"/fs/nexus-projects/sc_frag_len/nextflow/data/spliceu_ref/refdata-gex-GRCh38-2020-A/spliceu.fa\",\n",
    "    \"/fs/nexus-projects/sc_frag_len/nextflow/data/spliceu_ref/refdata-gex-GRCh38-2020-A/t2g_3col.tsv\",\n",
    "    \"1000\",\n",
    "    \"32\",\n",
    "    \"/fs/nexus-projects/sc_frag_len/nextflow/test/read_to_tx_prob\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmers(sequence, ksize):\n",
    "    kmers = []\n",
    "    n_kmers = len(sequence) - ksize + 1\n",
    "\n",
    "    for i in range(n_kmers):\n",
    "        kmer = sequence[i:(i + ksize)]\n",
    "        kmers.append(kmer)\n",
    "\n",
    "    return kmers\n",
    "\n",
    "def get_algn_to_tx_prob(algn, spline, mlp, ref_seq, encoder, max_frag_len=1000, polya_tail_len = 250, discount_perc = 0.75, snr_min_size=6, binding_affinity_threshold = 0, is_single_exon_tx=False):\n",
    "    # if this is a sense alignment w.r.t. the reference transcript\n",
    "    # we first calculate the distance of the reference end site to the downstream polyA sites that are within 1000 bases of the read alignment\n",
    "    # x = (start, end) - polyA\n",
    "    # sense reads\n",
    "    #     --read-->                  \n",
    "    # tx ---------------------------[polyA]-------------------[polyA]--> 3'\n",
    "    #                    --sliding window--\n",
    "    \n",
    "    #                               --sliding window--\n",
    "    #                    |          | <- final reasult window\n",
    "    #\n",
    "    #                --sliding window--\n",
    "    #     --sliding window--\n",
    "    #   [polyT]      [polyT]           <--reads--\n",
    "    #                      |          | <- final reasult window\n",
    "    joint_prob = 0\n",
    "    best_pos = 0\n",
    "    # frag_len_weight = 0.5\n",
    "    # discount_perc = 1 if algn[\"reference_name\"].endswith(\"-U\") else discount_perc\n",
    "    if not algn[\"is_reverse\"]:\n",
    "        ref_start = algn['reference_end'] - algn[\"query_length\"]\n",
    "        \n",
    "        if algn[\"cigartuples\"][-1][0] == 4:\n",
    "            ref_start += algn[\"cigartuples\"][-1][1]\n",
    "        \n",
    "        # if we have 30 bases downstream, we want to consider internal polyA sites\n",
    "        # we use > 30 because the polyA should start one base after the read alignment\n",
    "        if len(ref_seq) - ref_start > 30:\n",
    "            # we want to take the 1000 30 k-mers, so it's 1030 bases\n",
    "            # polyA site has to be at least 1 base away from ref start so that the ref length is positive\n",
    "            downstream_30mers = build_kmers(ref_seq[(ref_start+1):min(ref_start + max_frag_len + 30, len(ref_seq))], 30)\n",
    "            # we only compute the binding affinity of the 30-mers that contain AAA\n",
    "            has_triple_a = [\"A\" * snr_min_size in x for x in downstream_30mers]\n",
    "            if has_triple_a and sum(has_triple_a) > 0:\n",
    "                \n",
    "                # first, we want to compute the fragment length probability of each 30-mer\n",
    "                downstream_frag_len_prob = interpolate.splev(\n",
    "                    range(\n",
    "                        1,\n",
    "                        len(downstream_30mers) + 1\n",
    "                    ), \n",
    "                    spline\n",
    "                )\n",
    "                \n",
    "                # we compute the binding affinity of the 30-mers that contain AAA\n",
    "                nonzero_binding_affinity = mlp.predict_proba(encoder.fit_transform([list(x) for i, x in enumerate(downstream_30mers) if has_triple_a[i]]))[:,1] * discount_perc\n",
    "                nonzero_binding_affinity = np.array([x if x > binding_affinity_threshold else 0 for x in nonzero_binding_affinity])\n",
    "                # we initialize the binding affinity as a small number\n",
    "                downstream_binding_affinity = np.zeros(len(downstream_30mers)) # + min(nonzero_binding_affinity)/2\n",
    "                downstream_binding_affinity[np.array(has_triple_a)] = nonzero_binding_affinity\n",
    "                # joint_probs = downstream_binding_affinity*(1-frag_len_weight) + downstream_frag_len_prob * frag_len_weight\n",
    "                joint_probs = downstream_binding_affinity * downstream_frag_len_prob\n",
    "                \n",
    "                # we want to discount the priming window probagblity to distinguish internal polyA and termial polyA\n",
    "                joint_prob = joint_probs.max()\n",
    "                best_pos = joint_probs.argmax() + 1\n",
    "        \n",
    "        # if we reach the end of the reference,\n",
    "        # we want to consider the polyA tail\n",
    "        \n",
    "        # ----> read\n",
    "        # tx ------------------------> 3'\n",
    "        #           |<--------->| <- start of the last 30-mer\n",
    "        #                           start pos of the last 30-mer - ref_start\n",
    "        # The distance from reference start to the last kmer start\n",
    "        dis_to_tx_end_30mer = len(ref_seq) - 30 + 1 - ref_start\n",
    "        \n",
    "        # if the distance to the tail is less than the max window size, \n",
    "        # then we want to consider polyA tail\n",
    "        # farg length range is 0-999, but the value is 1000\n",
    "        # TODO: @Rob: Do we need to discount the tail probability for single exon transcripts, say lncRNAs?\n",
    "        if dis_to_tx_end_30mer < max_frag_len and not is_single_exon_tx:\n",
    "            # we first compute the fragment length\n",
    "            tail_joint_prob = interpolate.splev(\n",
    "                range(\n",
    "                    # one base after the start pos of the last 30-mer in the ref\n",
    "                    dis_to_tx_end_30mer + 1, \n",
    "                    min(max_frag_len, dis_to_tx_end_30mer + 1 + polya_tail_len) + 1\n",
    "                ), \n",
    "                spline\n",
    "            ) #  * frag_len_weight\n",
    "            # tail_binding_affinity = np.ones(min(max_frag_len - 30 + 1 - dist_to_tail, 200))\n",
    "            tail_30mers =  build_kmers(ref_seq[-(30 - 1):] + 'A' * min(max_frag_len - dis_to_tx_end_30mer, 29),30)\n",
    "            # for others, we use the binding affinity of all A\n",
    "            # all_a_prob = mlp.predict_proba(encoder.fit_transform([list('A'*30)]))[:,1]\n",
    "            all_a_prob = 1\n",
    "            tail_binding_affinity = mlp.predict_proba(encoder.fit_transform([list(x) for x in tail_30mers]))[:,1]\n",
    "            tail_binding_affinity = np.array([x if x > binding_affinity_threshold else 0 for x in tail_binding_affinity])\n",
    "\n",
    "            # tail_binding_affinity = np.array([1 if x > 0.7 else 0 for x in tail_binding_affinity])\n",
    "\n",
    "            tail_joint_prob[:len(tail_30mers)] = tail_joint_prob[:len(tail_30mers)] * tail_binding_affinity # + tail_binding_affinity * (1-frag_len_weight)\n",
    "            tail_joint_prob[len(tail_30mers):] = tail_joint_prob[len(tail_30mers):] * all_a_prob # + all_a_prob #* (1-frag_len_weight)\n",
    "            \n",
    "            max_tail_joint_prob = tail_joint_prob.max()\n",
    "            # if we get the maximum probability from the tail, we set the best position to be infinity\n",
    "            if max_tail_joint_prob > joint_prob:\n",
    "                joint_prob = max_tail_joint_prob\n",
    "                best_pos = float('inf')\n",
    "#      >>>>>>>\n",
    "#-------AAAAAAAAA\n",
    "    else:\n",
    "        # now we have a antisense alignment\n",
    "        \"\"\"\n",
    "        antisense reads\n",
    "            read                                                                       <----------\n",
    "              tx   ========================================================================================================> 3'\n",
    "                                                      [polyT)\n",
    "                                       ------------------------------------\n",
    "        \"\"\"\n",
    "        \n",
    "        ref_end = algn[\"reference_start\"] + algn['query_length']\n",
    "        \n",
    "        if algn[\"cigartuples\"][0][0] == 4:\n",
    "            ref_end += algn['cigartuples'][0][1]\n",
    "\n",
    "        # if we have 30 bases upstream, we want to consider internal polyT sites\n",
    "        if ref_end > 30:\n",
    "            # we take the upstream 1000 30 k-mers\n",
    "            upstream_30mers = build_kmers(ref_seq[max(ref_end - max_frag_len - 30, 0):ref_end - 1].reverse_complement(), 30)\n",
    "            \n",
    "            has_triple_a = [\"A\" * snr_min_size in x for x in upstream_30mers]\n",
    "            if has_triple_a and sum(has_triple_a) > 0:\n",
    "                # first, we want to compute the fragment length probability of each 30-mer\n",
    "                upstream_frag_len_prob = interpolate.splev(\n",
    "                    range(\n",
    "                        1,\n",
    "                        1 + len(upstream_30mers)\n",
    "                    ), \n",
    "                    spline\n",
    "                )\n",
    "                \n",
    "                # we initialize the binding affinity as zero\n",
    "                # we compute the binding affinity of the 30-mers that contain AAA\n",
    "                nonzero_binding_affinity = mlp.predict_proba(encoder.fit_transform([list(x) for i, x in enumerate(upstream_30mers) if has_triple_a[i]]))[:,1] * discount_perc\n",
    "                nonzero_binding_affinity = np.array([x if x > binding_affinity_threshold else 0 for x in nonzero_binding_affinity])\n",
    "\n",
    "                upstream_binding_affinity = np.zeros(len(upstream_30mers)) # + min(nonzero_binding_affinity)/2\n",
    "                upstream_binding_affinity[np.array(has_triple_a)] = nonzero_binding_affinity\n",
    "\n",
    "                # joint_probs = upstream_binding_affinity * (1-frag_len_weight) + upstream_frag_len_prob * frag_len_weight\n",
    "                joint_probs = upstream_binding_affinity * upstream_frag_len_prob\n",
    "                \n",
    "                # we want to discount the priming window probagblity to distinguish internal polyA and termial polyA\n",
    "                joint_prob = joint_probs.max()\n",
    "                best_pos = joint_probs.argmax() + 1\n",
    "    \n",
    "    # Then we return the maximum probability\n",
    "    return joint_prob, best_pos\n",
    "\n",
    "def algn_to_tx_prob_parallel(txome_bam, spline, mlp, spliceu_txome, max_frag_len, num_threads, polya_tail_len = 250, discount_perc = 1, snr_min_size = 3, binding_affinity_threshold = 0):\n",
    "    \"\"\"\n",
    "    This function is used to parallelize the algn_probs function\n",
    "    It takes\n",
    "    1. a pysam.AlignmentFile object from the transcriptome BAM file\n",
    "    2. the polyA and polyT sites on each transcript\n",
    "    3. the spline model\n",
    "    4. the MLP model\n",
    "    5. the spliceu reference sequence\n",
    "    6. the polyA minimum length\n",
    "    7. the maximum fragment length\n",
    "    8. the number of threads to use\n",
    "    \n",
    "    It returns a list of tuples, where each tuple is \\\\\n",
    "    (read name, reference id, is sense, maximum joint probability of arising from the reference).\\\\\n",
    "    Each alignment in the BAM file will be converted to a tuple after computing the probability of arising from the reference.\n",
    "    \"\"\"\n",
    "    # Use joblib's Parallel to parallelize predictions\n",
    "    # cannot use prefer=\"threads\" as it will move the variable into the thread\n",
    "    \n",
    "    def algn_to_tx_prob(algn, spline, mlp, ref_seq, encoder, max_frag_len, polya_tail_len, discount_perc, snr_min_size, binding_affinity_threshold, is_single_exon_tx):\n",
    "        prob, best_pos = get_algn_to_tx_prob(algn, spline, mlp, ref_seq, encoder, max_frag_len, polya_tail_len, discount_perc, snr_min_size, binding_affinity_threshold, is_single_exon_tx)\n",
    "        return (algn['query_name'], algn['reference_id'], not algn['is_reverse'], prob)\n",
    "\n",
    "    encoder = OneHotEncoder(categories=[['A', 'C', 'G', 'T', 'N']] * 30, handle_unknown='ignore')\n",
    "    start_time = time.time()\n",
    "    algn_to_tx_prob = Parallel(n_jobs=num_threads, prefer=None)(\n",
    "        delayed(algn_to_tx_prob)(\n",
    "            {\"is_reverse\": algn.is_reverse, \n",
    "            \"query_length\": algn.query_length, \n",
    "            \"cigartuples\": algn.cigartuples, \n",
    "            \"reference_end\": algn.reference_end, \n",
    "            \"reference_start\": algn.reference_start, \n",
    "            \"reference_name\": algn.reference_name, \n",
    "            \"query_name\": algn.query_name,\n",
    "            \"reference_id\": algn.reference_id\n",
    "            }, \n",
    "            spline, \n",
    "            mlp, \n",
    "            spliceu_txome.get(algn.reference_name), \n",
    "            encoder,\n",
    "            # args.max_frag_len) for algn in algns[:1000] if not algn.is_unmapped\n",
    "            max_frag_len,\n",
    "            polya_tail_len,\n",
    "            discount_perc,\n",
    "            snr_min_size,\n",
    "            binding_affinity_threshold,\n",
    "            # True if not algn.reference_name.endswith(\"-U\") and algn.reference_name.strip(\"-U\") + \"-U\" not in spliceu_txome else False\n",
    "            False\n",
    "            # ) for (rid,algn) in enumerate(txome_bam_records) if not algn.is_unmapped\n",
    "            ) for (rid,algn) in enumerate(txome_bam.fetch(until_eof=True)) if not algn.is_unmapped\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed the BAM file in {elapsed_time} seconds\")\n",
    "    \n",
    "    return (algn_to_tx_prob)\n",
    "\n",
    "def read_to_gene_orienataion_prob(algn_to_tx_prob, tid2g, tid2tname):\n",
    "    \"\"\"\n",
    "    This function takes the alignment to transctipt probability returned from `algn_to_tx_prob_parallel` and convert it to read to gene probability.\n",
    "    The input object is a list of tuples, where each tuple is \\\\\n",
    "    (read name, reference id, is sense, probability of arising from the reference). \\\\\n",
    "    Each read in the BAM file will be converted to a tuple after summarizing all its alignments.\n",
    "    This function outputs two dictionaries. The first dictionary contains the read to gene probability for each reference gene. The second dictionary contains the final predicted splicing status and the correspondingprobablity .\n",
    "    In the first dictionary, \n",
    "    \n",
    "    \"\"\"\n",
    "    # keys: read name\n",
    "    # values: {gid: [2*[unspliced_prob_sum, spliced_prob_sum, max_unspliced_prob, max_spliced_prob, , num_unspliced, num_spliced]]}\n",
    "    # where the first list is for sense alignments and the second list is for antisense alignments\n",
    "    reads_predictions = {it[0]: {} for it in algn_to_tx_prob}\n",
    "    for rname, tid, is_sense, prob in algn_to_tx_prob:\n",
    "        gid = tid2g[tid]\n",
    "                \n",
    "        # if this is a new reference gene \n",
    "        if gid not in reads_predictions[rname]:\n",
    "            # if we see the same best probability, we add it in\n",
    "            reads_predictions[rname][gid] = [[0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0]]\n",
    "        \n",
    "        # if the probability is zero, we skip it\n",
    "        if prob == 0:\n",
    "            continue\n",
    "        \n",
    "        # we define the index of the list we want to modify\n",
    "        is_sense = int(is_sense)\n",
    "        is_spliced = int(not tid2tname[tid].endswith(\"-U\"))\n",
    "        # we update the sum probability\n",
    "        reads_predictions[rname][gid][is_sense][is_spliced] += prob\n",
    "        # we update the number of appearances\n",
    "        reads_predictions[rname][gid][is_sense][is_spliced+4] += 1\n",
    "        # we update the max probability\n",
    "        if prob > reads_predictions[rname][gid][is_sense][is_spliced+2]:\n",
    "            reads_predictions[rname][gid][is_sense][is_spliced+2] = prob\n",
    "\n",
    "    return reads_predictions\n",
    "\n",
    "def read_to_gene_prob(reads_predictions, splicing_status_min_ratio = 1, antisense_min_ratio = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    This step takes the read to gene probability returned from `read_to_gene_prob` and convert it to read to gene probability.\n",
    "    Input: a dictionary of\n",
    "    {\n",
    "        read_name: {\n",
    "            gid: [\n",
    "                [antisense_spliced_prob_sum, antisense_unspliced_prob_sum, antisense_max_spliced_prob, antisense_max_unspliced_prob, num_unspliced, num_spliced],\n",
    "                [sense_spliced_prob_sum, sense_unspliced_prob_sum, sense_max_spliced_prob, sense_max_unspliced_prob, num_unspliced, num_spliced]\n",
    "            ]\n",
    "        }\n",
    "    } \n",
    "    Output: a dictionary of\n",
    "    {\n",
    "        read_name: [[\n",
    "            orientation,\n",
    "            splicing_status,\n",
    "        ]]\n",
    "            \n",
    "    \"\"\"\n",
    "    def get_splicing_status(prob_list,min_ratio):\n",
    "        # [sum, max, num]\n",
    "        is_spliced = 0\n",
    "        splicing_status = \"A\"\n",
    "        s_prob = 0.5\n",
    "        u_prob = 0.5\n",
    "        \n",
    "        if sum(prob_list) == 0:\n",
    "            return(splicing_status, 0, 0, (0.5, 0.5))\n",
    "        \n",
    "        # get mean from sum (the first twos), and count (the third twos)\n",
    "        mean_u_prob = prob_list[0]/prob_list[4] if prob_list[4] != 0 else 0\n",
    "        mean_s_prob = prob_list[1]/prob_list[5] if prob_list[5] != 0 else 0\n",
    "        max_u_prob = prob_list[2]\n",
    "        max_s_prob = prob_list[3]\n",
    "        \n",
    "        # if sum probs are the same, we use the winner of the max probabilities\n",
    "        if max_u_prob != max_s_prob:\n",
    "            is_spliced = int(max_s_prob > max_u_prob)\n",
    "            splicing_status = \"S\" if is_spliced == 1 else \"U\"\n",
    "            denominator = max_s_prob + max_u_prob\n",
    "            s_prob = max_s_prob/denominator\n",
    "            u_prob = max_u_prob/denominator\n",
    "        \n",
    "        # # if we have a winner, we use it\n",
    "        # elif mean_u_prob != mean_s_prob:\n",
    "        #     is_spliced = int(mean_s_prob > mean_u_prob)\n",
    "        #     splicing_status = \"S\" if is_spliced == 1 else \"U\"\n",
    "        #     denominator = mean_s_prob + mean_u_prob\n",
    "        #     s_prob = mean_s_prob/denominator\n",
    "        #     u_prob = mean_u_prob/denominator\n",
    "        \n",
    "        mean_prob = mean_s_prob if is_spliced == 1 else mean_u_prob\n",
    "        max_prob = prob_list[is_spliced+2]\n",
    "        \n",
    "        # # finally, we want to change the splicing status back to ambiguous if the ratio is too small\n",
    "        # if mean_s_prob == 0 or mean_u_prob == 0:\n",
    "        #     mean_prob_ratio = min_ratio\n",
    "        # else:\n",
    "        #     mean_prob_ratio = mean_s_prob/mean_u_prob if is_spliced == 1 else mean_u_prob/mean_s_prob\n",
    "\n",
    "        if max_s_prob == 0 or max_u_prob == 0:\n",
    "            max_prob_ratio = min_ratio\n",
    "        else:\n",
    "            max_prob_ratio = max_s_prob/max_u_prob if is_spliced == 1 else max_u_prob/max_s_prob\n",
    "            \n",
    "        # if mean_prob_ratio < min_ratio and max_prob_ratio < min_ratio:\n",
    "        if max_prob_ratio < min_ratio:\n",
    "            splicing_status = \"A\"\n",
    "\n",
    "        return (splicing_status, mean_prob, max_prob, (u_prob, s_prob))\n",
    "    \n",
    "    reads_gene_probs = {rname: [] for rname in reads_predictions}\n",
    "    for rname, predictions in reads_predictions.items():\n",
    "        # predictions = {gid: [2*[unspliced_prob_sum, spliced_prob_sum, max_unspliced_prob, max_spliced_prob, num_unspliced, num_spliced]]}\n",
    "        for gid, probs in predictions.items():\n",
    "            # we first process antisense alignments\n",
    "            is_sense = int(False)\n",
    "            antisense_splicing_status, antisense_mean_prob, antisense_max_prob, antisense_prediction_prob = get_splicing_status(probs[is_sense], min_ratio=splicing_status_min_ratio)\n",
    "            \n",
    "            # we then process sense alignments\n",
    "            is_sense = int(True)\n",
    "            sense_splicing_status, sense_mean_prob, sense_max_prob, sense_prediction_prob = get_splicing_status(probs[is_sense], min_ratio=splicing_status_min_ratio)\n",
    "            \n",
    "            is_sense = int(False) # we might not have the final orientation\n",
    "            splicing_status = \"A\" # initialize as ambiguous\n",
    "            orientation = \"*\"\n",
    "            final_prediction_prob = (0.5, 0.5)\n",
    "            \n",
    "            # if we have a winner, we use it\n",
    "            # If we are totally ambiguous\n",
    "            if antisense_mean_prob == sense_mean_prob and antisense_max_prob == sense_max_prob:\n",
    "                reads_gene_probs[rname].append((gid, orientation, splicing_status, antisense_max_prob, final_prediction_prob))\n",
    "                continue\n",
    "            # if not, we use the winner of the max probabilities\n",
    "            if antisense_max_prob != sense_max_prob:\n",
    "                is_sense = int(sense_max_prob > antisense_max_prob/antisense_min_ratio)\n",
    "                splicing_status = sense_splicing_status if is_sense == 1 else antisense_splicing_status\n",
    "                final_prediction_prob = sense_prediction_prob if is_sense == 1 else antisense_prediction_prob\n",
    "                orientation = \"+\" if is_sense == 1 else \"-\"\n",
    "            elif antisense_mean_prob != sense_mean_prob:\n",
    "                # we consider it as antisense only if its antisense prob is 1.2 times larger than sense\n",
    "                is_sense = int(sense_mean_prob > antisense_mean_prob/antisense_min_ratio)\n",
    "                splicing_status = sense_splicing_status if is_sense == 1 else antisense_splicing_status\n",
    "                final_prediction_prob = sense_prediction_prob if is_sense == 1 else antisense_prediction_prob\n",
    "                orientation = \"+\" if is_sense == 1 else \"-\"\n",
    "\n",
    "            max_prob = sense_max_prob if is_sense == 1 else antisense_max_prob\n",
    "            \n",
    "            # Now we push the result to the dictionary\n",
    "            reads_gene_probs[rname].append((gid, orientation, splicing_status, max_prob, final_prediction_prob))\n",
    "            \n",
    "    return reads_gene_probs\n",
    "\n",
    "def read_gene_prediction(reads_gene_probs, r2g=None):\n",
    "    \"\"\"\n",
    "    This function takes the read to its reference genes' probability returned from `read_to_gene_prob` and convert it to read to gene prediction. We will return a list of references that the read is most likely to arise from. If there is a tie, we will return all the references.\n",
    "    Input: a dictionary of\n",
    "    {\n",
    "        read_name: [\n",
    "            (gid, orientation, splicing_status, max_prob, (antisense_prob, sense_prob))\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Output: a dictionary of\n",
    "    {\n",
    "        read_name: [\n",
    "            (gid, orientation, splicing_status, max_prob, (antisense_prob, sense_prob))\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    read_count = 0\n",
    "    unigene_count = 0\n",
    "    unigene_spliced_reads_count = 0\n",
    "    unigene_unspliced_reads_count = 0\n",
    "    unigene_ambiguous_reads_count = 0\n",
    "    unigene_wronggene_count = 0\n",
    "    multigene_count = 0\n",
    "    rescued_multigene_count = 0\n",
    "    rescued_wrong_gene_count = 0\n",
    "    rescued_spliced_reads_count = 0\n",
    "    rescued_unspliced_reads_count = 0\n",
    "    rescued_ambiguous_reads_count = 0\n",
    "    final_multi_gene_count = 0\n",
    "\n",
    "    read_gene_predictions = {}\n",
    "    for rname, predictions in reads_gene_probs.items():\n",
    "        # predictions : [(gid, orientation, splicing_status, max_prob, (antisense_prob, sense_prob))]\n",
    "        read_count += 1\n",
    "        # if we have a single gene, we just return it\n",
    "        if len(predictions) == 1:\n",
    "            read_gene_predictions[rname] = predictions\n",
    "            unigene_count += 1\n",
    "            # if the best gene is not the true gene, we count it as wrong\n",
    "            if r2g is not None and predictions[0][0] != r2g[rname]:\n",
    "                unigene_wronggene_count += 1\n",
    "            elif predictions[0][2] == \"S\":\n",
    "                unigene_spliced_reads_count += 1\n",
    "            elif predictions[0][2] == \"U\":\n",
    "                unigene_unspliced_reads_count += 1\n",
    "            else:\n",
    "                unigene_ambiguous_reads_count += 1\n",
    "        # now we need to do a prediction\n",
    "        else:\n",
    "            multigene_count += 1\n",
    "            # get genes' probability\n",
    "            gene_probs = [x[3] for x in predictions]\n",
    "            gene_oris = [x[1] for x in predictions]\n",
    "            max_prob = max(gene_probs)\n",
    "            # get the best genes\n",
    "            max_index = [i for i, x in enumerate(gene_probs) if x == max_prob]\n",
    "            # if we have a single best gene, then we return it\n",
    "            if len(max_index) == 1:\n",
    "                max_index = max_index[0]\n",
    "                read_gene_predictions[rname] = [predictions[max_index]]\n",
    "                rescued_multigene_count += 1\n",
    "                # if the best gene is not the true gene, we count it as wrong\n",
    "                if r2g is not None and predictions[max_index][0] != r2g[rname]:\n",
    "                    rescued_wrong_gene_count += 1\n",
    "                elif predictions[max_index][2] == \"S\":\n",
    "                    rescued_spliced_reads_count += 1\n",
    "                elif predictions[max_index][2] == \"U\":\n",
    "                    rescued_unspliced_reads_count += 1\n",
    "                else:\n",
    "                    rescued_ambiguous_reads_count += 1\n",
    "            # if we have multiple best genes, we return all of them\n",
    "            else:\n",
    "                final_multi_gene_count += 1\n",
    "                read_gene_predictions[rname] = [predictions[i] for i in max_index]\n",
    "    \n",
    "    log = {\n",
    "        \"read_count\" : read_count,\n",
    "        \"unigene_count\" : unigene_count,\n",
    "        \"unigene_wronggene_count\" : unigene_wronggene_count,\n",
    "        \"unigene_spliced_reads_count\" : unigene_spliced_reads_count,\n",
    "        \"unigene_unspliced_reads_count\" : unigene_unspliced_reads_count,\n",
    "        \"unigene_ambiguous_reads_count\" : unigene_ambiguous_reads_count,\n",
    "        \"multigene_count\" : multigene_count,\n",
    "        \"rescued_wrong_gene_count\" : rescued_wrong_gene_count,\n",
    "        \"rescued_multigene_count\" : rescued_multigene_count,\n",
    "        \"rescued_spliced_reads_count\" : rescued_spliced_reads_count,\n",
    "        \"rescued_unspliced_reads_count\" : rescued_unspliced_reads_count,\n",
    "        \"rescued_ambiguous_reads_count\" : rescued_ambiguous_reads_count,\n",
    "        \"final_multi_gene_count\" : final_multi_gene_count,\n",
    "        \"final_spliced_count\" : unigene_spliced_reads_count + rescued_spliced_reads_count,\n",
    "        \"final_unspliced_count\" : unigene_unspliced_reads_count + rescued_unspliced_reads_count,\n",
    "        \"final_ambiguous_count\" : unigene_ambiguous_reads_count + rescued_ambiguous_reads_count\n",
    "    }\n",
    "    \n",
    "    return read_gene_predictions, log\n",
    "\n",
    "def test(txpme_bam, polya_tail_len = 250, discount_perc = 1, snr_min_size = 6, binding_affinity_threshold = 0, splicing_status_min_ratio = 1, antisense_min_ratio = 1):\n",
    "    simulated_algn_to_tx_prob = algn_to_tx_prob_parallel(txpme_bam, spline, mlp, txome, args.max_frag_len, args.num_threads, polya_tail_len = polya_tail_len, discount_perc = discount_perc, snr_min_size = snr_min_size, binding_affinity_threshold = binding_affinity_threshold)\n",
    "    # simulated_reads_txome_bam.close()\n",
    "\n",
    "    simulated_read_to_gene_orientation_results = read_to_gene_orienataion_prob(simulated_algn_to_tx_prob, tid2g, tid2tname)\n",
    "    simulated_read_to_gene_results = read_to_gene_prob(simulated_read_to_gene_orientation_results, splicing_status_min_ratio = splicing_status_min_ratio, antisense_min_ratio = antisense_min_ratio)\n",
    "    simulated_read_gene_predicitons,simulated_read_gene_predicitons_log = read_gene_prediction(simulated_read_to_gene_results)\n",
    "    simulated_read_gene_predicitons_log\n",
    "\n",
    "    multimapped_reads = []\n",
    "    s2s = []\n",
    "    s2u = []\n",
    "    s2a = []\n",
    "    u2s = []\n",
    "    u2u = []\n",
    "    u2a = []\n",
    "    num_rescued = 0\n",
    "    num_reads = 0\n",
    "    num_wronggene = 0\n",
    "    for rname, predictions in simulated_read_gene_predicitons.items():\n",
    "        num_reads += 1\n",
    "        if len(predictions) == 1:\n",
    "            if predictions[0][0] != t2g[rname.split('-')[0]]:\n",
    "                num_wronggene += 1\n",
    "            elif predictions[0][2] == \"S\":\n",
    "                if  \"-U-\" in rname:\n",
    "                    u2s.append((rname, predictions))\n",
    "                else:\n",
    "                    s2s.append((rname, predictions))\n",
    "            elif predictions[0][2] == \"U\":\n",
    "                if  \"-U-\" in rname:\n",
    "                    u2u.append((rname, predictions))\n",
    "                else:\n",
    "                    s2u.append((rname, predictions))\n",
    "            else:\n",
    "                if  \"-U-\" in rname:\n",
    "                    u2a.append((rname, predictions))\n",
    "                else:\n",
    "                    s2a.append((rname, predictions))\n",
    "            \n",
    "            # check if the reads are \n",
    "        else:\n",
    "            # get genes' probability\n",
    "            gene_probs = [x[3] for x in predictions]\n",
    "            gene_oris = [x[1] for x in predictions]\n",
    "            max_prob = max(gene_probs)\n",
    "            # get the best genes\n",
    "            max_index = [i for i, x in enumerate(gene_probs) if x == max_prob]\n",
    "            # if we have a single best gene, then we return it\n",
    "            if len(max_index) == 1:\n",
    "                num_rescued += 1\n",
    "                max_index = max_index[0]\n",
    "                \n",
    "                if predictions[max_index][0] != t2g[rname.split('-')[0]]:\n",
    "                    num_wronggene += 1\n",
    "                if predictions[max_index][2] == \"S\":\n",
    "                    if  \"-U-\" in rname:\n",
    "                        u2s.append((rname, predictions))\n",
    "                    else:\n",
    "                        s2s.append((rname, predictions))\n",
    "                elif predictions[max_index][2] == \"U\":\n",
    "                    if  \"-U-\" in rname:\n",
    "                        u2u.append((rname, predictions))\n",
    "                    else:\n",
    "                        s2u.append((rname, predictions))\n",
    "                else:\n",
    "                    if  \"-U-\" in rname:\n",
    "                        u2a.append((rname, predictions))\n",
    "                    else:\n",
    "                        s2a.append((rname, predictions))\n",
    "            # if we have multiple best genes, we return all of them\n",
    "            else:\n",
    "                multimapped_reads.append((rname, predictions))\n",
    "    \n",
    "    print(f\"num of reads: {num_reads}\")\n",
    "    print(f\"num of reads assignged to wrong gene: {num_wronggene}\")\n",
    "    print(f\"num of final multimapped reads: {len(multimapped_reads)}\") \n",
    "    print(f\"num of rescued multimapped reads: {num_rescued}\")\n",
    "    print(f\"num of s2s reads: {len(s2s)}\")\n",
    "    print(f\"num of s2u reads: {len(s2u)}\")\n",
    "    print(f\"num of s2a reads: {len(s2a)}\")\n",
    "    print(f\"num of s: {len(s2s) + len(s2u) + len(s2a)}\")\n",
    "    print(f\"num of u2s reads: {len(u2s)}\")\n",
    "    print(f\"num of u2u reads: {len(u2u)}\")\n",
    "    print(f\"num of u2a reads: {len(u2a)}\")\n",
    "    print(f\"num of u: {len(u2s) + len(u2u) + len(u2a)}\")\n",
    "    return num_reads, num_rescued, s2s, s2u, s2a, u2s, u2u, u2a, multimapped_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/fs/nexus-projects/sc_frag_len/nextflow/fit_model/Jan24_out/spline_model.pkl\", 'rb') as file_model:\n",
    "    spline = pickle.load(file_model)\n",
    "\n",
    "# read in the mlp model\n",
    "# with open(args.mlp_model_file, 'rb') as file_model:\n",
    "with open(\"/fs/nexus-projects/sc_frag_len/nextflow/test/mlp/mlp_pa6with1mm.pkl\", 'rb') as file_model:\n",
    "    mlp = pickle.load(file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2g_path = \"/fs/nexus-projects/sc_frag_len/nextflow/data/spliceu_ref/refdata-gex-GRCh38-2020-A/t2g_3col.tsv\"\n",
    "txome_path = \"/fs/nexus-projects/sc_frag_len/nextflow/data/spliceu_ref/refdata-gex-GRCh38-2020-A/spliceu.fa\"\n",
    "\n",
    "# Read in the t2g file\n",
    "t2g_df = pd.read_csv(t2g_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"tid\", \"gid\", \"splicing_status\"])\n",
    "t2g = t2g_df.set_index(\"tid\").to_dict()[\"gid\"]\n",
    "t2s = t2g_df.set_index(\"tid\").to_dict()[\"splicing_status\"]\n",
    "\n",
    "# read in the spliceu fasta file\n",
    "txome = {record.id: record.seq for record in SeqIO.parse(\n",
    "    txome_path, \"fasta\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_reads_txome_bam = pysam.AlignmentFile(\"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simulated_data_allow_fundamental_ambiguity/processed/simu_STAR_out/se_Aligned.toTranscriptome.out.bam\", \"rb\")\n",
    "tid2tlen = dict(zip(range(len(simulated_reads_txome_bam.references)), simulated_reads_txome_bam.lengths))\n",
    "tid2tname = dict(zip(range(len(simulated_reads_txome_bam.references)), simulated_reads_txome_bam.references))\n",
    "tid2g = {tid: t2g[tname] for tid, tname in tid2tname.items()}\n",
    "\n",
    "num_reads, num_rescued, s2s, s2u, s2a, u2s, u2u, u2a, multimapped_reads = test(simulated_reads_txome_bam, polya_tail_len = 200, discount_perc = 1, snr_min_size = 6, binding_affinity_threshold = 0, splicing_status_min_ratio = 1, antisense_min_ratio = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_labels = np.hstack([\n",
    "    np.ones(len(s2s)),\n",
    "    np.ones(len(s2u)),\n",
    "    np.ones(len(s2a)),\n",
    "    np.zeros(len(u2s)),\n",
    "    np.zeros(len(u2u)),\n",
    "    np.zeros(len(u2a))\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs = np.hstack([\n",
    "    np.array([p[1][0][4][1] for p in s2s]),\n",
    "    np.array([p[1][0][4][1] for p in s2u]),\n",
    "    np.array([p[1][0][4][1] for p in s2a]),\n",
    "    np.array([p[1][0][4][1] for p in u2s]),\n",
    "    np.array([p[1][0][4][1] for p in u2u]),\n",
    "    np.array([p[1][0][4][1] for p in u2a])\n",
    "])\n",
    "\n",
    "read_probs_all_spliced = np.hstack([\n",
    "    np.ones(len(s2s)),\n",
    "    np.ones(len(s2u)),\n",
    "    np.ones(len(s2a)),\n",
    "    np.ones(len(u2s)),\n",
    "    np.ones(len(u2u)),\n",
    "    np.ones(len(u2a)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs_all_unspliced = np.hstack([\n",
    "    np.zeros(len(s2s)),\n",
    "    np.zeros(len(s2u)),\n",
    "    np.zeros(len(s2a)),\n",
    "    np.zeros(len(u2s)),\n",
    "    np.zeros(len(u2u)),\n",
    "    np.zeros(len(u2a)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs_rand = np.hstack([\n",
    "    [random.uniform(0.5, 1) for _ in range(sum(read_labels== 1))], [random.uniform(0, 0.5) for _ in range(sum(read_labels== 0))]\n",
    "])\n",
    "random.shuffle( read_probs_rand )\n",
    "\n",
    "auc_as = roc_auc_score(read_labels, read_probs_all_spliced)\n",
    "fpr_as, tpr_as, _ = roc_curve(read_labels, read_probs_all_spliced)\n",
    "\n",
    "auc_au = roc_auc_score(read_labels, read_probs_all_unspliced)\n",
    "fpr_au, tpr_au, _ = roc_curve(read_labels, read_probs_all_unspliced)\n",
    "\n",
    "auc_rand = roc_auc_score(read_labels, read_probs_rand)\n",
    "fpr_rand, tpr_rand, _ = roc_curve(read_labels, read_probs_rand)\n",
    "\n",
    "auc = roc_auc_score(read_labels, read_probs)\n",
    "fpr, tpr, _ = roc_curve(read_labels, read_probs)\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.plot(fpr, tpr, marker = \".\", markersize = 4, linewidth = 1, label='Forseti (AUC = %0.2f)' % auc)\n",
    "plt.plot(fpr_as, tpr_as, marker = \"x\", markersize = 4, linewidth = 1, label='All spliced (AUC = %0.2f)' % auc_as)\n",
    "plt.plot(fpr_au, tpr_au, marker = \"+\", markersize = 4, linewidth = 1, label='All unspliced (AUC = %0.2f)' % auc_au)\n",
    "plt.plot(fpr_rand, tpr_rand, marker = \"1\", markersize = 4, linewidth = 1, label='All random (AUC = %0.2f)' % auc_rand)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend(fontsize = 13)\n",
    "auroc_path = os.path.join(\"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simulated_data_allow_fundamental_ambiguity/processed\", 'simulated_data_allow_fundamental_ambiguity_model_predictions.jpg')\n",
    "plt.savefig(auroc_path, format=\"jpg\", bbox_inches=\"tight\", dpi=500)\n",
    "auroc_path = os.path.join(\"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simulated_data_allow_fundamental_ambiguity/processed\", 'simulated_data_allow_fundamental_ambiguity_model_predictions.pdf')\n",
    "plt.savefig(auroc_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC without ambiguous predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_labels = np.hstack([\n",
    "    np.ones(len(s2s)),\n",
    "    np.ones(len(s2u)),\n",
    "    np.zeros(len(u2s)),\n",
    "    np.zeros(len(u2u)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs = np.hstack([\n",
    "    np.array([p[1][0][4][1] for p in s2s]),\n",
    "    np.array([p[1][0][4][1] for p in s2u]),\n",
    "    np.array([p[1][0][4][1] for p in u2s]),\n",
    "    np.array([p[1][0][4][1] for p in u2u]),\n",
    "])\n",
    "\n",
    "read_probs_all_spliced = np.hstack([\n",
    "    np.ones(len(s2s)),\n",
    "    np.ones(len(s2u)),\n",
    "    np.ones(len(u2s)),\n",
    "    np.ones(len(u2u)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs_all_unspliced = np.hstack([\n",
    "    np.zeros(len(s2s)),\n",
    "    np.zeros(len(s2u)),\n",
    "    np.zeros(len(u2s)),\n",
    "    np.zeros(len(u2u)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "read_probs_rand = np.hstack([\n",
    "    [random.uniform(0.5, 1) for _ in range(sum(read_labels== 1))], [random.uniform(0, 0.5) for _ in range(sum(read_labels== 0))]\n",
    "])\n",
    "random.shuffle( read_probs_rand )\n",
    "\n",
    "\n",
    "auc_as = roc_auc_score(read_labels, read_probs_all_spliced)\n",
    "fpr_as, tpr_as, _ = roc_curve(read_labels, read_probs_all_spliced)\n",
    "\n",
    "auc_au = roc_auc_score(read_labels, read_probs_all_unspliced)\n",
    "fpr_au, tpr_au, _ = roc_curve(read_labels, read_probs_all_unspliced)\n",
    "\n",
    "auc_rand = roc_auc_score(read_labels, read_probs_rand)\n",
    "fpr_rand, tpr_rand, _ = roc_curve(read_labels, read_probs_rand)\n",
    "\n",
    "auc = roc_auc_score(read_labels, read_probs)\n",
    "fpr, tpr, _ = roc_curve(read_labels, read_probs)\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.plot(fpr, tpr, marker = \".\", markersize = 4, linewidth = 1, label='Forseti (AUC = %0.2f)' % auc)\n",
    "plt.plot(fpr_as, tpr_as, marker = \"x\", markersize = 4, linewidth = 1, label='All spliced (AUC = %0.2f)' % auc_as)\n",
    "plt.plot(fpr_au, tpr_au, marker = \"+\", markersize = 4, linewidth = 1, label='All unspliced (AUC = %0.2f)' % auc_au)\n",
    "plt.plot(fpr_rand, tpr_rand, marker = \"1\", markersize = 4, linewidth = 1, label='All random (AUC = %0.2f)' % auc_rand)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend(fontsize = 13)\n",
    "auroc_path = os.path.join(\"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simulated_data_allow_fundamental_ambiguity/processed\", 'simulated_data_allow_fundamental_ambiguity_model_predictions_ambiguous_prediction_excluded.pdf')\n",
    "plt.savefig(auroc_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "auroc_path = os.path.join(\"/fs/nexus-projects/sc_frag_len/nextflow/simulation/simulated_data_allow_fundamental_ambiguity/processed\", 'simulated_data_allow_fundamental_ambiguity_model_predictions_ambiguous_prediction_excluded.jpg')\n",
    "plt.savefig(auroc_path, format=\"jpg\", bbox_inches=\"tight\", dpi=500)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
