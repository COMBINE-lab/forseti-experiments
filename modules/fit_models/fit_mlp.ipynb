{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import regex\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "This script trains a MLP classifier to predict the binding affinity of a 30 bases sequence w.r.t. the obtained priming window.\n",
    "\n",
    "The training data is obtained from the upstream 30 bases of the alignments of R1 in our paired-end read alignment results from the genome. The training data has the same orientation as its read2, so, it should contain a polyA site.\n",
    "As reads might come from polyA tails, which are not included in the genome, we would expect that some of the input data come from intergenic regions and do not correspond to a polyA site. This is saying our labels are noisy, so we would not expect to obtain perfect accuracy.\n",
    "\n",
    "We train a MLP classifier by reading the training data from each SRR and call `partial_fit` to update the model. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser = json.load(open(\"params.json\", \"r\"))\n",
    "parent_dir = parser[\"parent_dir\"]\n",
    "PE_sheet = parser[\"PE_sheet\"]\n",
    "outdir = parser[\"outdir\"]\n",
    "random_seed = parser[\"random_seed\"]\n",
    "snr_len = parser[\"snr_len\"]\n",
    "snr_mismatch = parser[\"snr_mismatch\"]\n",
    "\n",
    "outdir = outdir\n",
    "print(parser)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = OneHotEncoder(categories=[['A', 'C', 'G', 'T', 'N']] * 30, handle_unknown='ignore')\n",
    "\n",
    "parent_dir = parent_dir\n",
    "# 1----- Get the PE datasets spreadsheet\n",
    "PE_sheet = pd.read_csv(PE_sheet)\n",
    "\n",
    "# 2----- loop through GSE(s), combine all tlen from its SRR\n",
    "# check if we have all datasets processed\n",
    "missing_files = []\n",
    "\n",
    "for (GSE, group_gse_lst) in PE_sheet.groupby('GSE'):\n",
    "    SRR_lst = group_gse_lst['SRR']\n",
    "    for SRR in SRR_lst:\n",
    "        polya_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "\n",
    "        if os.path.exists(polya_path):\n",
    "            check_file = os.path.getsize(polya_path)\n",
    "            if (check_file == 0):\n",
    "                missing_files.append(f\"{GSE}-{SRR}\")\n",
    "                error_occur = True\n",
    "        else:\n",
    "            missing_files.append(f\"{GSE}-{SRR}\")\n",
    "            error_occur = True\n",
    "\n",
    "if missing_files:\n",
    "    raise ValueError(f\"Please re-run the previous step, the output of following dataset(s) is either missing or empty: {missing_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polya_mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, random_state=random_seed, shuffle=True,\n",
    "                    learning_rate_init=.001)\n",
    "\n",
    "held_out_polya = sp.csr_matrix((0, 150))\n",
    "held_out_polya_bg = sp.csr_matrix((0, 150)) \n",
    "\n",
    "random.seed(random_seed)\n",
    "train_list = PE_sheet.loc[PE_sheet['GSE'].isin(random.sample(PE_sheet['GSE'].unique().tolist(), 8))]\n",
    "num_srr = {GSE: group_gse_lst.shape[0] for (GSE, group_gse_lst) in train_list.groupby('GSE')}\n",
    "# initialize the sparse matrix \n",
    "training_batch = sp.csr_matrix((0, 150))\n",
    "held_out_polya_bg = sp.csr_matrix((0, 150)) \n",
    "\n",
    "training_data_acc_list = []\n",
    "\n",
    "# we group by GSE, then loop through SRRs to get the polya seq\n",
    "for i, (GSE, group_gse_lst) in enumerate(train_list.groupby('GSE')):\n",
    "    SRR_lst = group_gse_lst['SRR']\n",
    "    num_held_out = 1000//len(SRR_lst)\n",
    "    \n",
    "    for SRR in SRR_lst:\n",
    "        # The 30 bases are defined according to R2. So, they should always be polyA\n",
    "\n",
    "        polya_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "        bg_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_bg_seq.txt\")\n",
    "\n",
    "        # get polya and bg seq\n",
    "        # we want to make sure the polya seq has a polyA six mer with at most one mismatch\n",
    "        polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30 and regex.search(\"A(\" + 'A' * (snr_len-2)+ \"){s<=\" + str(snr_mismatch) +\"}A\", x.strip())])\n",
    "        # polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30])\n",
    "        \n",
    "        # if we do not have enough data, then skip this SRR\n",
    "        if polya_batch.shape[0] < num_held_out * 2:\n",
    "            print(\"Not enough training examples for\", GSE, SRR)\n",
    "            continue\n",
    "        \n",
    "        bg_batch = encoder.fit_transform([list(x.strip()) for x in open(bg_path).readlines() if len(x.strip()) == 30])\n",
    "        \n",
    "        # if we do not have enough data, then skip this SRR\n",
    "        if bg_batch.shape[0] < num_held_out * 2:\n",
    "            print(\"Not enough background examples for\", GSE, SRR)\n",
    "            continue\n",
    "        \n",
    "        # num_held_out = min(math.ceil(polya_batch.shape[0] * 0.1), 20)\n",
    "        num_train_fg = polya_batch.shape[0]\n",
    "        num_train_bg = min(num_train_fg, bg_batch.shape[0])\n",
    "\n",
    "        polya_batch = polya_batch[np.random.randint(0, polya_batch.shape[0], size = num_train_fg), : ]\n",
    "        bg_batch = bg_batch[np.random.randint(0, bg_batch.shape[0], size = num_train_bg), : ]\n",
    "\n",
    "        # append held out data\n",
    "        polya_held_out = polya_batch[:num_held_out,]\n",
    "        bg_held_out = bg_batch[:num_held_out,]\n",
    "        \n",
    "        held_out_polya = sp.vstack([held_out_polya, polya_held_out])\n",
    "        held_out_polya_bg = sp.vstack([held_out_polya_bg, bg_held_out])\n",
    "        \n",
    "        fg_shuf_id = np.arange((num_train_fg+num_train_bg-num_held_out*2))\n",
    "        np.random.shuffle(fg_shuf_id)\n",
    "\n",
    "        # build training data\n",
    "        train_polya = sp.vstack([\n",
    "            polya_batch[num_held_out:],\n",
    "            bg_batch[num_held_out:]\n",
    "        ])[fg_shuf_id,:]\n",
    "        \n",
    "        del polya_batch\n",
    "        del bg_batch\n",
    "        \n",
    "        label_polya = np.hstack([\n",
    "            np.ones(num_train_fg - num_held_out), \n",
    "            np.zeros(num_train_bg - num_held_out)\n",
    "        ])[fg_shuf_id]\n",
    "        \n",
    "        # fit the model using the data from this SRR\n",
    "        polya_mlp.partial_fit(\n",
    "            train_polya,\n",
    "            label_polya,\n",
    "            classes=[0,1]\n",
    "        )\n",
    "        \n",
    "        acc = accuracy_score(\n",
    "            np.hstack(\n",
    "                [np.ones(num_held_out), \n",
    "                np.zeros(num_held_out)\n",
    "                ]\n",
    "            ), \n",
    "            polya_mlp.predict(\n",
    "                sp.vstack([\n",
    "                    polya_held_out, \n",
    "                    bg_held_out]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        training_data_acc_list.append(acc)\n",
    "        # get the accuracy on the held out data\n",
    "        print(\"Accuracy:\", acc\n",
    "        )\n",
    "\n",
    "print(\"The mean accuracy on the training datasets is:\", sum(training_data_acc_list)/len(training_data_acc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finally, we want to get the overall accuracy on all held out data\n",
    "predictions = polya_mlp.predict(sp.vstack([held_out_polya, held_out_polya_bg]) )\n",
    "print(\"The mean accuracy on the holdout training examples is :\", accuracy_score(np.hstack([np.ones(held_out_polya.shape[0]), np.zeros(held_out_polya_bg.shape[0])]), predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we a;sp also want to test the accuracy on test data\n",
    "# we read in the test data\n",
    "test_list = PE_sheet.loc[~PE_sheet['GSE'].isin(random.sample(PE_sheet['GSE'].unique().tolist(), 8))]\n",
    "test_acc_list = []\n",
    "# we group by GSE, then loop through SRRs to get the polya seq\n",
    "for i, (GSE, group_gse_lst) in enumerate(test_list.groupby('GSE')):\n",
    "    SRR_lst = group_gse_lst['SRR']\n",
    "    for SRR in SRR_lst:\n",
    "        # The 30 bases are defined according to R2. So, they should always be polyA\n",
    "\n",
    "        polya_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "        bg_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_bg_seq.txt\")\n",
    "\n",
    "        # get polya and bg seq\n",
    "        # we want to make sure the polya seq has a polyA six mer with at most one mismatch\n",
    "        polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30 and regex.search(\"A(\" + 'A' * (snr_len-2)+ \"){s<=\" + str(snr_mismatch) +\"}A\", x.strip())])\n",
    "        # polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30])\n",
    "        \n",
    "        # if we do not have enough data, then skip this SRR\n",
    "        if polya_batch.shape[0] < num_held_out * 2:\n",
    "            print(\"Not enough training examples for\", GSE, SRR)\n",
    "            continue\n",
    "        \n",
    "        bg_batch = encoder.fit_transform([list(x.strip()) for x in open(bg_path).readlines() if len(x.strip()) == 30])\n",
    "        \n",
    "        # if we do not have enough data, then skip this SRR\n",
    "        if bg_batch.shape[0] < num_held_out * 2:\n",
    "            print(\"Not enough background examples for\", GSE, SRR)\n",
    "            continue\n",
    "        \n",
    "        # build training data\n",
    "        train_polya = sp.vstack([\n",
    "            polya_batch[random.sample(range(polya_batch.shape[0]), min(polya_batch.shape[0], bg_batch.shape[0])), : ],\n",
    "            bg_batch[random.sample(range(bg_batch.shape[0]), min(polya_batch.shape[0], bg_batch.shape[0])), : ]\n",
    "        ])\n",
    "        label_polya = np.hstack([\n",
    "            np.ones(min(polya_batch.shape[0], bg_batch.shape[0])), \n",
    "            np.zeros(min(polya_batch.shape[0], bg_batch.shape[0]))\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        del polya_batch\n",
    "        del bg_batch\n",
    "        \n",
    "        # get the accuracy on the held out data\n",
    "        acc_scores = accuracy_score(\n",
    "            label_polya, \n",
    "            polya_mlp.predict(\n",
    "                train_polya\n",
    "            )\n",
    "        )\n",
    "        test_acc_list.append(acc_scores)\n",
    "        \n",
    "        print(\"Accuracy:\", acc_scores)\n",
    "\n",
    "\n",
    "print(\"The mean accuracy on the test datasets is:\", sum(test_acc_list)/len(test_acc_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pkl_path = os.path.join(outdir, 'mlp.pkl')\n",
    "with open(model_pkl_path, 'wb') as file_model:\n",
    "    pickle.dump(polya_mlp, file_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
