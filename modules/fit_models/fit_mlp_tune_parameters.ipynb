{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T02:09:48.428363Z",
     "iopub.status.busy": "2024-03-13T02:09:48.428091Z",
     "iopub.status.idle": "2024-03-13T02:09:51.927282Z",
     "shell.execute_reply": "2024-03-13T02:09:51.926723Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import random\n",
    "import regex\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\"\"\"\n",
    "This script trains a MLP classifier to predict the binding affinity of a 30 bases sequence w.r.t. the obtained priming window.\n",
    "\n",
    "The training data is obtained from the upstream 30 bases of the alignments of R1 in our paired-end read alignment results from the genome. The training data has the same orientation as its read2, so, it should contain a polyA site.\n",
    "As reads might come from polyA tails, which are not included in the genome, we would expect that some of the input data come from intergenic regions and do not correspond to a polyA site. This is saying our labels are noisy, so we would not expect to obtain perfect accuracy.\n",
    "\n",
    "We train a MLP classifier by reading the training data from each SRR and call `partial_fit` to update the model. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser = json.load(open(\"params.json\", \"r\"))\n",
    "parent_dir = parser[\"parent_dir\"]\n",
    "PE_sheet = parser[\"PE_sheet\"]\n",
    "random_seed = parser[\"random_seed\"]\n",
    "outdir_parent = parser[\"outdir\"]\n",
    "snr_len = parser[\"snr_len\"]\n",
    "snr_mismatch = parser[\"snr_mismatch\"]\n",
    "\n",
    "print(parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T02:09:51.955100Z",
     "iopub.status.busy": "2024-03-13T02:09:51.954507Z",
     "iopub.status.idle": "2024-03-13T02:09:52.374817Z",
     "shell.execute_reply": "2024-03-13T02:09:52.374314Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder = OneHotEncoder(categories=[['A', 'C', 'G', 'T', 'N']] * 30, handle_unknown='ignore')\n",
    "\n",
    "parent_dir = parent_dir\n",
    "# 1----- Get the PE datasets spreadsheet\n",
    "PE_sheet = pd.read_csv(PE_sheet)\n",
    "\n",
    "# 2----- loop through GSE(s), combine all tlen from its SRR\n",
    "# check if we have all datasets processed\n",
    "missing_files = []\n",
    "\n",
    "for (GSE, group_gse_lst) in PE_sheet.groupby('GSE'):\n",
    "    SRR_lst = group_gse_lst['SRR']\n",
    "    print(GSE)\n",
    "    for SRR in SRR_lst:\n",
    "        polya_path = os.path.join(\n",
    "            parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "\n",
    "        if os.path.exists(polya_path):\n",
    "            check_file = os.path.getsize(polya_path)\n",
    "            if (check_file == 0):\n",
    "                missing_files.append(f\"{GSE}-{SRR}\")\n",
    "                error_occur = True\n",
    "        else:\n",
    "            missing_files.append(f\"{GSE}-{SRR}\")\n",
    "            error_occur = True\n",
    "\n",
    "if missing_files:\n",
    "    raise ValueError(f\"Please re-run the previous step, the output of following dataset(s) is either missing or empty: {missing_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T02:09:52.377681Z",
     "iopub.status.busy": "2024-03-13T02:09:52.377198Z",
     "iopub.status.idle": "2024-03-13T04:16:59.208198Z",
     "shell.execute_reply": "2024-03-13T04:16:59.207751Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## first run are interruped.\n",
    "hidden_layer_sizes_list = [50,100]\n",
    "max_iter_list = [100,200,500]\n",
    "\n",
    "\n",
    "for hidden_layer_sizes in hidden_layer_sizes_list:\n",
    "    for max_iter in max_iter_list:\n",
    "        start_time = time.time()\n",
    "        outdir = os.path.join(outdir_parent,f'/seed{random_seed}_layer_{hidden_layer_sizes}_iter_{max_iter}') \n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # ------------------- Train the MLP model -------------------\n",
    "        polya_mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes,), max_iter=max_iter, alpha=1e-4,\n",
    "                            solver='adam', verbose=10, random_state=random_seed, shuffle=True,\n",
    "                            learning_rate_init=0.001)\n",
    "\n",
    "        held_out_polya = sp.csr_matrix((0, 150))\n",
    "        held_out_polya_bg = sp.csr_matrix((0, 150)) \n",
    "        random.seed(random_seed)\n",
    "        train_list = PE_sheet.loc[PE_sheet['GSE'].isin(random.sample(PE_sheet['GSE'].unique().tolist(), 8))]\n",
    "        num_srr = {GSE: group_gse_lst.shape[0] for (GSE, group_gse_lst) in train_list.groupby('GSE')}\n",
    "        # initialize the sparse matrix \n",
    "        training_batch = sp.csr_matrix((0, 150))\n",
    "\n",
    "        training_data_acc_list_during_train = []\n",
    "\n",
    "        # we group by GSE, then loop through SRRs to get the polya seq\n",
    "        for i, (GSE, group_gse_lst) in enumerate(train_list.groupby('GSE')):\n",
    "            SRR_lst = group_gse_lst['SRR']\n",
    "            num_held_out = 1000//len(SRR_lst)\n",
    "            \n",
    "            for SRR in SRR_lst:\n",
    "                # The 30 bases are defined according to R2. So, they should always be polyA\n",
    "\n",
    "                polya_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "                bg_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_bg_seq.txt\")\n",
    "\n",
    "                # get polya and bg seq\n",
    "                # we want to make sure the polya seq has a polyA six mer with at most one mismatch\n",
    "                polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30 and regex.search(\"A(\" + 'A' * (snr_len-2)+ \"){s<=\" + str(snr_mismatch) +\"}A\", x.strip())])\n",
    "\n",
    "                # polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if polya_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough training examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                bg_batch = encoder.fit_transform([list(x.strip()) for x in open(bg_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if bg_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough background examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                # num_held_out = min(math.ceil(polya_batch.shape[0] * 0.1), 20)\n",
    "                num_train_fg = polya_batch.shape[0]\n",
    "                num_train_bg = min(num_train_fg, bg_batch.shape[0])\n",
    "\n",
    "                polya_batch = polya_batch[np.random.randint(0, polya_batch.shape[0], size = num_train_fg), : ]\n",
    "                bg_batch = bg_batch[np.random.randint(0, bg_batch.shape[0], size = num_train_bg), : ]\n",
    "                \n",
    "                fg_shuf_id = np.arange((num_train_fg+num_train_bg-num_held_out*2))\n",
    "                np.random.shuffle(fg_shuf_id)\n",
    "\n",
    "                # build training data\n",
    "                train_polya = sp.vstack([\n",
    "                    polya_batch[num_held_out:],\n",
    "                    bg_batch[num_held_out:]\n",
    "                ])[fg_shuf_id,:]\n",
    "                \n",
    "                del polya_batch\n",
    "                del bg_batch\n",
    "                \n",
    "                label_polya = np.hstack([\n",
    "                    np.ones(num_train_fg - num_held_out), \n",
    "                    np.zeros(num_train_bg - num_held_out)\n",
    "                ])[fg_shuf_id]\n",
    "                \n",
    "                # fit the model using the data from this SRR\n",
    "                polya_mlp.partial_fit(\n",
    "                    train_polya,\n",
    "                    label_polya,\n",
    "                    classes=[0,1]\n",
    "                )\n",
    "                acc = accuracy_score(label_polya, \n",
    "                    polya_mlp.predict(train_polya))\n",
    "                training_data_acc_list_during_train.append(acc)\n",
    "                # get the accuracy on the held out data\n",
    "                print(\"Accuracy:\", acc)\n",
    "\n",
    "\n",
    "        print(\"The mean accuracy on the training datasets is:\", sum(training_data_acc_list_during_train)/len(training_data_acc_list_during_train))\n",
    "\n",
    "        # ------------------- Test the model(partI)-------------------\n",
    "        # Test the model on the training sets and held out sets for each srr\n",
    "        random.seed(random_seed)\n",
    "        train_list = PE_sheet.loc[PE_sheet['GSE'].isin(random.sample(PE_sheet['GSE'].unique().tolist(), 8))]\n",
    "        num_srr = {GSE: group_gse_lst.shape[0] for (GSE, group_gse_lst) in train_list.groupby('GSE')}\n",
    "        # initialize the sparse matrix \n",
    "        training_batch = sp.csr_matrix((0, 150))\n",
    "        held_out_polya = sp.csr_matrix((0, 150))\n",
    "        held_out_polya_bg = sp.csr_matrix((0, 150)) \n",
    "\n",
    "        training_data_acc_list = []\n",
    "        held_out_data_acc_list = []\n",
    "\n",
    "        # we group by GSE, then loop through SRRs to get the polya seq\n",
    "        for i, (GSE, group_gse_lst) in enumerate(train_list.groupby('GSE')):\n",
    "            SRR_lst = group_gse_lst['SRR']\n",
    "            num_held_out = 1000//len(SRR_lst)\n",
    "            # inital the matrix/list for each GSE\n",
    "            train_polya_per_gse = sp.csr_matrix((0, 150))\n",
    "            train_label_per_gse = np.array([])\n",
    "\n",
    "            held_out_polya_per_gse = sp.csr_matrix((0, 150))\n",
    "            held_out_label_per_gse = np.array([])\n",
    "\n",
    "            for SRR in SRR_lst:\n",
    "                # The 30 bases are defined according to R2. So, they should always be polyA\n",
    "                polya_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "                bg_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_bg_seq.txt\")\n",
    "\n",
    "                # get polya and bg seq\n",
    "                # we want to make sure the polya seq has a polyA six mer with at most one mismatch\n",
    "                polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30 and regex.search(\"A(\" + 'A' * (snr_len-2)+ \"){s<=\" + str(snr_mismatch) +\"}A\", x.strip())])\n",
    "                \n",
    "                # polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if polya_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough training examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                bg_batch = encoder.fit_transform([list(x.strip()) for x in open(bg_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if bg_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough background examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                # num_held_out = min(math.ceil(polya_batch.shape[0] * 0.1), 20)\n",
    "                num_train_fg = polya_batch.shape[0]\n",
    "                num_train_bg = min(num_train_fg, bg_batch.shape[0])\n",
    "\n",
    "                polya_batch = polya_batch[np.random.randint(0, polya_batch.shape[0], size = num_train_fg), : ]\n",
    "                bg_batch = bg_batch[np.random.randint(0, bg_batch.shape[0], size = num_train_bg), : ]\n",
    "\n",
    "                # append held out data\n",
    "                polya_held_out = polya_batch[:num_held_out,]\n",
    "                bg_held_out = bg_batch[:num_held_out,]\n",
    "                \n",
    "                fg_shuf_id = np.arange((num_train_fg+num_train_bg-num_held_out*2))\n",
    "                np.random.shuffle(fg_shuf_id)\n",
    "\n",
    "                # build training data\n",
    "                train_polya = sp.vstack([\n",
    "                    polya_batch[num_held_out:],\n",
    "                    bg_batch[num_held_out:]\n",
    "                ])[fg_shuf_id,:]\n",
    "                \n",
    "                del polya_batch\n",
    "                del bg_batch\n",
    "                \n",
    "                label_polya = np.hstack([\n",
    "                    np.ones(num_train_fg - num_held_out), \n",
    "                    np.zeros(num_train_bg - num_held_out)\n",
    "                ])[fg_shuf_id]\n",
    "\n",
    "                # group the train data by gse\n",
    "                train_polya_per_gse = sp.vstack([train_polya_per_gse, train_polya])\n",
    "                train_label_per_gse = np.hstack([train_label_per_gse, label_polya])\n",
    "\n",
    "                #group the held out data by gse\n",
    "                held_out_label_per_gse = np.hstack([held_out_label_per_gse, np.hstack([np.ones(num_held_out), np.zeros(num_held_out)])])\n",
    "                held_out_polya_per_gse = sp.vstack([held_out_polya_per_gse, sp.vstack([polya_held_out, bg_held_out])])\n",
    "\n",
    "            # accuracy for classification on the training data\n",
    "            acc_train = accuracy_score(train_label_per_gse, \n",
    "                polya_mlp.predict(train_polya_per_gse))\n",
    "            training_data_acc_list.append(acc_train)\n",
    "            print(\"Accuracy on training data:\", acc_train)\n",
    "\n",
    "            # accuracy for classification on the held out data\n",
    "            acc_held_out = accuracy_score(held_out_label_per_gse, \n",
    "                polya_mlp.predict(held_out_polya_per_gse))\n",
    "            held_out_data_acc_list.append(acc_held_out)\n",
    "            print(\"Accuracy on held out data:\", acc_held_out)\n",
    "\n",
    "        print(\"The mean accuracy on the training datasets is:\", sum(training_data_acc_list)/len(training_data_acc_list))\n",
    "        print(\"The mean accuracy on the held out datasets is:\", sum(held_out_data_acc_list)/len(held_out_data_acc_list))\n",
    "\n",
    "        # --------- Test the model (partII)-----------------\n",
    "        # we also want to test the accuracy on test datasets\n",
    "        # we read in the test data\n",
    "        random.seed(random_seed)\n",
    "        test_list = PE_sheet.loc[~PE_sheet['GSE'].isin(random.sample(PE_sheet['GSE'].unique().tolist(), 8))]\n",
    "        test_acc_list = []\n",
    "        # we group by GSE, then loop through SRRs to get the polya seq\n",
    "        for i, (GSE, group_gse_lst) in enumerate(test_list.groupby('GSE')):\n",
    "            SRR_lst = group_gse_lst['SRR']\n",
    "\n",
    "            test_polya_per_gse = sp.csr_matrix((0, 150))\n",
    "            test_label_per_gse = np.array([])\n",
    "            \n",
    "            for SRR in SRR_lst:\n",
    "                # The 30 bases are defined according to R2. So, they should always be polyA\n",
    "\n",
    "                polya_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_seq.txt\")\n",
    "                bg_path = os.path.join(\n",
    "                    parent_dir, \"process_data\", \"frag_len_dist\", GSE, SRR, \"priming_site_seqs\", \"polya_bg_seq.txt\")\n",
    "\n",
    "                # get polya and bg seq\n",
    "                # we want to make sure the polya seq has a polyA six mer with at most one mismatch\n",
    "                polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30 and regex.search(\"A(\" + 'A' * (snr_len-2)+ \"){s<=\" + str(snr_mismatch) +\"}A\", x.strip())])\n",
    "                # polya_batch = encoder.fit_transform([list(x.strip()) for x in open(polya_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if polya_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough training examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                bg_batch = encoder.fit_transform([list(x.strip()) for x in open(bg_path).readlines() if len(x.strip()) == 30])\n",
    "                \n",
    "                # if we do not have enough data, then skip this SRR\n",
    "                if bg_batch.shape[0] < num_held_out * 2:\n",
    "                    print(\"Not enough background examples for\", GSE, SRR)\n",
    "                    continue\n",
    "                \n",
    "                # build training data\n",
    "                test_polya = sp.vstack([\n",
    "                    polya_batch[random.sample(range(polya_batch.shape[0]), min(polya_batch.shape[0], bg_batch.shape[0])), : ],\n",
    "                    bg_batch[random.sample(range(bg_batch.shape[0]), min(polya_batch.shape[0], bg_batch.shape[0])), : ]\n",
    "                ])\n",
    "                label_polya = np.hstack([\n",
    "                    np.ones(min(polya_batch.shape[0], bg_batch.shape[0])), \n",
    "                    np.zeros(min(polya_batch.shape[0], bg_batch.shape[0]))\n",
    "                ])\n",
    "\n",
    "                del polya_batch\n",
    "                del bg_batch\n",
    "\n",
    "                # group the test data by gse\n",
    "                test_polya_per_gse = sp.vstack([test_polya_per_gse, test_polya])\n",
    "                test_label_per_gse = np.hstack([test_label_per_gse, label_polya])\n",
    "                \n",
    "                \n",
    "            # get the accuracy on the held out data\n",
    "            acc_scores = accuracy_score(\n",
    "                test_label_per_gse, \n",
    "                polya_mlp.predict(test_polya_per_gse))\n",
    "            test_acc_list.append(acc_scores)\n",
    "            print(\"Accuracy:\", acc_scores)\n",
    "\n",
    "\n",
    "        print(\"The mean accuracy on the test datasets is:\", sum(test_acc_list)/len(test_acc_list))\n",
    "\n",
    "        # outdir = '/fs/nexus-projects/sc_frag_len/nextflow/workflow_output/models/mlp_model'\n",
    "        accuracy_file_path = os.path.join(outdir, \"accuracy_3_lists.pkl\")\n",
    "        joblib.dump([training_data_acc_list, held_out_data_acc_list, test_acc_list], accuracy_file_path)\n",
    "\n",
    "        [training_data_acc_list, held_out_data_acc_list, test_acc_list]= joblib.load(accuracy_file_path, \"rb\")\n",
    "        # accuracy log\n",
    "        acc_dict = {\n",
    "            \"training_data_acc_list\": training_data_acc_list,\n",
    "            \"held_out_data_acc_list\": held_out_data_acc_list,\n",
    "            \"test_acc_list\": test_acc_list\n",
    "        }\n",
    "        with open(os.path.join(outdir, 'acc_list_log.txt'),'w') as file:\n",
    "            file.write(f\"The time used for training the model is: {time.time() - start_time} seconds\\n\")\n",
    "            for l_name, acc_list in acc_dict.items():\n",
    "                file.write( '---------'+ \"\\n\")\n",
    "                file.write(l_name + \"\\n\")\n",
    "                for acc_num in acc_list:\n",
    "                        file.write(str(acc_num)+\"\\n\")\n",
    "        file.close()\n",
    "        \n",
    "        #------------------- Save the model -------------------\n",
    "        model_pkl_path = os.path.join(outdir, f'mlp.pkl')\n",
    "        joblib.dump(polya_mlp, model_pkl_path)\n",
    "\n",
    "        # Combine the lists into a single dataset with corresponding labels\n",
    "        data = training_data_acc_list + held_out_data_acc_list + test_acc_list\n",
    "        labels = ['Training'] * len(training_data_acc_list) + ['Hold out'] * len(held_out_data_acc_list) + ['Test'] * len(test_acc_list)\n",
    "        print(f\"training_data_acc_list: {training_data_acc_list}\")\n",
    "        print(f\"held_out_data_acc_list: {held_out_data_acc_list}\")\n",
    "        print(f\"test_acc_list: {test_acc_list}\")\n",
    "        # Create a DataFrame for easier plotting with Seaborn\n",
    "        df = pd.DataFrame({'Data': data, 'Datasets': labels})\n",
    "\n",
    "        # Create the violin plot\n",
    "        sns.violinplot(x='Datasets', y='Data', data=df)\n",
    "        plt.title(\"Distribution of prediction accuracy of MLP\")\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.savefig(os.path.join(outdir, f\"violin_plot_per_gse.png\"),dpi = 300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
